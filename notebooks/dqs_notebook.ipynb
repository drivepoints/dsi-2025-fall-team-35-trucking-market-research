{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Data Quality Score (DQS) Pipeline\n",
    "\n",
    "This notebook automates the complete DQS calculation workflow:\n",
    "1. Download latest USDOT Motor Carrier Census data\n",
    "2. Convert to Parquet format\n",
    "3. Calculate DQS metrics (completeness, validity, timeliness)\n",
    "4. Run LLM-powered semantic validity checks on sample\n",
    "5. Store results in historical tracking CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sodapy import Socrata\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All packages imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../data\n",
      "Results directory: ../quality-scores\n",
      "Key fields tracked: 27\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RESULTS_DIR = Path(\"../quality-scores\")\n",
    "HISTORY_FILE = RESULTS_DIR / \"dqs_history.csv\"\n",
    "\n",
    "# Use existing, or fetch the latest dataset from US DOT FMCSA (takes 2 minutes to download)\n",
    "USE_EXISTING_DATA=True\n",
    "\n",
    "# Socrata API settings\n",
    "SOCRATA_DOMAIN = \"data.transportation.gov\"\n",
    "DATASET_ID = \"kjg3-diqy\"\n",
    "FETCH_LIMIT = 5000000  # Fetch all records\n",
    "\n",
    "# DQS Settings\n",
    "KEY_FIELDS = [\n",
    "    \"dot_number\", \"legal_name\", \"dba_name\", \"carrier_operation\",\n",
    "    \"us_mail\", \"authorized_for_hire\", \"private_only\", \"add_date\",\n",
    "    \"email_address\", \"telephone\", \"oic_state\", \"phy_street\",\n",
    "    \"phy_city\", \"phy_state\", \"phy_zip\", \"mailing_street\",\n",
    "    \"mailing_city\", \"mailing_state\", \"mailing_zip\", \"driver_total\",\n",
    "    \"nbr_power_unit\", \"mcs150_date\", \"mcs150_mileage\", \"mcs150_mileage_year\",\n",
    "    \"recent_mileage\", \"recent_mileage_year\", \"vmt_source_id\"\n",
    "]\n",
    "# Weights are configurable with initial weights set to: W1=0.4, W2=0.4, W3=0.2)\n",
    "WEIGHTS = {\"completeness\": 4/10, \"validity\": 4/10, \"timeliness\": 2/10}\n",
    "\n",
    "# LLM Settings\n",
    "LLM_SAMPLE_SIZE = 10\n",
    "LLM_MODEL = \"o3-mini\"\n",
    "LLM_SEED = 27\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Key fields tracked: {len(KEY_FIELDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "download_header",
   "metadata": {},
   "source": [
    "## Step 1: Download Latest USDOT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "download",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, timestamp=None):\n",
    "    \"\"\"Save DataFrame to Parquet with timestamp.\"\"\"\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"transportation_data_{timestamp}.parquet\"\n",
    "    filepath = DATA_DIR / filename\n",
    "    print(f\"Saving to Parquet: {filename}...\")\n",
    "    df.to_parquet(filepath, index=False, compression='snappy')\n",
    "    file_size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "    print(f\"Saved {len(df):,} records ({file_size_mb:.1f} MB)\")\n",
    "    return filepath\n",
    "\n",
    "def fetch_usdot_data(use_existing_data):\n",
    "    \"\"\"Load from existing parquet if available, or download from Socrata.\"\"\"\n",
    "    existing_files = sorted(DATA_DIR.glob(\"transportation_data_*.parquet\"), reverse=True)\n",
    "    \n",
    "    # Use existing file if flag is set and file exists\n",
    "    if use_existing_data and existing_files:\n",
    "        latest_file = existing_files[0]\n",
    "        print(f\"Using existing data file: {latest_file.name}\")\n",
    "        df = pd.read_parquet(latest_file)\n",
    "        print(f\"Loaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "        return df, latest_file\n",
    "\n",
    "    # Otherwise fetch the latest dataset\n",
    "    print(f\"Connecting to {SOCRATA_DOMAIN}...\")\n",
    "    client = Socrata(SOCRATA_DOMAIN, None)\n",
    "    print(f\"Downloading dataset {DATASET_ID} (limit={FETCH_LIMIT:,})...\")\n",
    "    print(\"This will take 2-3 minutes...\")\n",
    "    results = client.get(DATASET_ID, limit=FETCH_LIMIT)\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "    print(f\"Downloaded {len(df):,} records with {len(df.columns)} columns\")\n",
    "\n",
    "    # Save to parquet\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    parquet_path = save_to_parquet(df, timestamp)\n",
    "    return df, parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convert_header",
   "metadata": {},
   "source": [
    "## Step 2: Convert and Save to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "convert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing data file: transportation_data_20251013_135544.parquet\n",
      "Loaded 2,091,643 records with 42 columns\n",
      "\n",
      "File location: ../data/transportation_data_20251013_135544.parquet\n",
      "\n",
      "Sample columns: ['dot_number', 'legal_name', 'dba_name', 'carrier_operation', 'hm_flag', 'pc_flag', 'phy_street', 'phy_city', 'phy_state', 'phy_zip']...\n"
     ]
    }
   ],
   "source": [
    "raw_df, parquet_path = fetch_usdot_data(use_existing_data=USE_EXISTING_DATA)\n",
    "print(f\"\\nFile location: {parquet_path}\")\n",
    "print(f\"\\nSample columns: {raw_df.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dqs_header",
   "metadata": {},
   "source": [
    "## Step 3: Calculate DQS Metrics\n",
    "\n",
    "Calculate completeness, structural validity, and timeliness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dqs_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQS calculation functions defined\n"
     ]
    }
   ],
   "source": [
    "def calc_completeness(df, key_fields=None):\n",
    "    \"\"\"Percentage of non-missing values in key fields.\"\"\"\n",
    "    # Remove duplicates from missing_vals\n",
    "    missing_vals = [\"\", \" \", \"  \", \"   \", \"NA\", \"N/A\", \"na\", \"n/a\", \"na.\", \n",
    "                    \"none\", \"None\", \"null\", \"NULL\", \"nan\", \"NaN\", \"missing\", \n",
    "                    \"Missing\", \"-\", \"--\"]\n",
    "    \n",
    "    # Use all columns if key_fields not specified\n",
    "    if key_fields is None:\n",
    "        key_fields = df.columns.tolist()\n",
    "    \n",
    "    # Calculate missing values: both standard NaN and custom missing strings\n",
    "    def is_missing(col):\n",
    "        # Standard missing values (NaN, None)\n",
    "        standard_missing = col.isna()\n",
    "        # Custom missing value strings (convert to string first to avoid type errors)\n",
    "        custom_missing = col.astype(str).isin(missing_vals)\n",
    "        # Combine both\n",
    "        return (standard_missing | custom_missing).mean()\n",
    "    \n",
    "    # Calculate completeness for each key field\n",
    "    missing_rate = df[key_fields].apply(is_missing)\n",
    "    \n",
    "    # Return average completeness across all key fields\n",
    "    return 1 - missing_rate.mean()\n",
    "\n",
    "def calc_structural_validity(df):\n",
    "    \"\"\"\n",
    "    Calculate structural validity (format checks) for key transportation data fields.\n",
    "    Returns the average validity across all performed checks.\n",
    "    \"\"\"\n",
    "    checks = []\n",
    "\n",
    "    # Telephone Numbers\n",
    "    if \"telephone\" in df.columns:\n",
    "        phone_pattern = r\"^\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}$\"\n",
    "        phone_valid = df[\"telephone\"].fillna(\"\").astype(str).str.match(phone_pattern)\n",
    "        checks.append(phone_valid.mean())\n",
    "\n",
    "    # US ZIP codes (5-digit or 5+4 format)\n",
    "    if \"phy_zip\" in df.columns:\n",
    "        zip_pattern = r\"^\\d{5}(?:-\\d{4})?$\"\n",
    "        zip_valid = df[\"phy_zip\"].fillna(\"\").astype(str).str.match(zip_pattern)\n",
    "        checks.append(zip_valid.mean())\n",
    "\n",
    "    # Email\n",
    "    if \"email_address\" in df.columns:\n",
    "        email_pattern = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "        email_valid = df[\"email_address\"].fillna(\"\").astype(str).str.match(email_pattern)\n",
    "        checks.append(email_valid.mean())\n",
    "\n",
    "    # US_DOT Number format (6-8 digits)\n",
    "    if \"dot_number\" in df.columns:\n",
    "        dot_str = df[\"dot_number\"].astype(str)\n",
    "        dot_pattern = r\"^\\d{6,8}$\"\n",
    "        dot_valid = dot_str.str.match(dot_pattern)\n",
    "        checks.append(dot_valid.mean())\n",
    "\n",
    "    # Date column format (DD-MON-YY)\n",
    "    if \"mcs150_date\" in df.columns:\n",
    "        date_pattern = r\"^\\d{2}-[A-Z]{3}-\\d{2}$\"\n",
    "        date_valid = df[\"mcs150_date\"].fillna(\"\").astype(str).str.match(date_pattern)\n",
    "        checks.append(date_valid.mean())\n",
    "\n",
    "    # State abbreviation format (exactly two uppercase letters)\n",
    "    if \"phy_state\" in df.columns:\n",
    "        state_pattern = r\"^[A-Z]{2}$\"\n",
    "        state_valid = df[\"phy_state\"].fillna(\"\").astype(str).str.match(state_pattern)\n",
    "        checks.append(state_valid.mean())\n",
    "\n",
    "    # Non-negative integer values for counts\n",
    "    for col in [\"nbr_power_unit\", \"driver_total\"]:\n",
    "        if col in df.columns:\n",
    "            # Step 1: Attempt to convert the column to numeric. Non-numeric strings become NaN.\n",
    "            numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "            # Step 2: Define validity criteria on the numeric column:\n",
    "            # 1. Not NaN (handles both original NaNs and strings that failed coercion)\n",
    "            is_valid = (~numeric_col.isna())\n",
    "            # 2. Non-negative\n",
    "            is_valid &= (numeric_col >= 0)\n",
    "            # 3. Has no fractional part (is an integer)\n",
    "            is_valid &= (numeric_col == np.floor(numeric_col))\n",
    "            checks.append(is_valid.mean())\n",
    "    return np.mean(checks) if checks else 0.0\n",
    "\n",
    "def calc_freshness(df, reference_date=None):\n",
    "    \"\"\"\n",
    "    Improved freshness calculation with weighted scoring and robust statistics.\n",
    "    \"\"\"\n",
    "    if reference_date is None:\n",
    "        reference_date = pd.Timestamp(datetime.today())\n",
    "    \n",
    "    current_year = reference_date.year\n",
    "    weighted_scores = []\n",
    "    \n",
    "    # PRIMARY: MCS-150 date (50% weight) - regulatory compliance indicator\n",
    "    if \"mcs150_date\" in df.columns:\n",
    "        mcs = pd.to_datetime(df[\"mcs150_date\"], format=\"%d-%b-%y\", errors=\"coerce\")\n",
    "        valid_mcs = mcs.dropna()\n",
    "        \n",
    "        if len(valid_mcs) > 0:\n",
    "            # Use median instead of mean to reduce outlier impact\n",
    "            median_age_days = (reference_date - valid_mcs).dt.days.median()\n",
    "            # 2-year threshold (730 days) for regulatory compliance\n",
    "            mcs_score = np.clip(1 - (median_age_days / 730), 0, 1)\n",
    "            weighted_scores.append((mcs_score, 0.5))\n",
    "    \n",
    "    # SECONDARY: MCS-150 mileage year (25% weight)\n",
    "    if \"mcs150_mileage_year\" in df.columns:\n",
    "        mcs_years = pd.to_numeric(df[\"mcs150_mileage_year\"], errors=\"coerce\")\n",
    "        # Explicitly filter out 0 values (not just NaN)\n",
    "        valid_mcs_years = mcs_years[mcs_years > 0]\n",
    "        \n",
    "        if len(valid_mcs_years) > 0:\n",
    "            median_year_diff = (current_year - valid_mcs_years).median()\n",
    "            # 3-year threshold (more lenient than date field)\n",
    "            mcs_year_score = np.clip(1 - (median_year_diff / 3), 0, 1)\n",
    "            weighted_scores.append((mcs_year_score, 0.25))\n",
    "    \n",
    "    # TERTIARY: Recent mileage year (15% weight)\n",
    "    if \"recent_mileage_year\" in df.columns:\n",
    "        recent_years = pd.to_numeric(df[\"recent_mileage_year\"], errors=\"coerce\")\n",
    "        # Critical: exclude 0 values which represent missing data\n",
    "        valid_recent = recent_years[recent_years > 0]\n",
    "        \n",
    "        if len(valid_recent) > 0:\n",
    "            median_year_diff = (current_year - valid_recent).median()\n",
    "            recent_score = np.clip(1 - (median_year_diff / 3), 0, 1)\n",
    "            weighted_scores.append((recent_score, 0.15))\n",
    "    \n",
    "    # QUATERNARY: Add date (10% weight) - less important for freshness\n",
    "    if \"add_date\" in df.columns:\n",
    "        adds = pd.to_datetime(df[\"add_date\"], format=\"%d-%b-%y\", errors=\"coerce\")\n",
    "        valid_adds = adds.dropna()\n",
    "        \n",
    "        if len(valid_adds) > 0:\n",
    "            median_age_days = (reference_date - valid_adds).dt.days.median()\n",
    "            # 10-year threshold (registration age, not freshness indicator)\n",
    "            add_score = np.clip(1 - (median_age_days / 3650), 0, 1)\n",
    "            weighted_scores.append((add_score, 0.1))\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    if weighted_scores:\n",
    "        total_score = sum(score * weight for score, weight in weighted_scores)\n",
    "        total_weight = sum(weight for _, weight in weighted_scores)\n",
    "        return total_score / total_weight\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def calc_dqs(completeness, validity, timeliness, weights=WEIGHTS):\n",
    "    \"\"\"Calculate overall DQS from component scores.\"\"\"\n",
    "    return (weights[\"completeness\"] * completeness +\n",
    "            weights[\"validity\"] * validity +\n",
    "            weights[\"timeliness\"] * timeliness)\n",
    "\n",
    "print(\"DQS calculation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "calculate_dqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating DQS metrics...\n",
      "\n",
      "Completeness: 0.924\n",
      "Structural Validity: 0.952\n",
      "Freshness: 0.254\n",
      "\n",
      "Base DQS: 0.801\n"
     ]
    }
   ],
   "source": [
    "# Calculate DQS metrics\n",
    "print(\"Calculating DQS metrics...\\n\")\n",
    "\n",
    "completeness_score = calc_completeness(raw_df)\n",
    "print(f\"Completeness: {completeness_score:.3f}\")\n",
    "\n",
    "validity_score = calc_structural_validity(raw_df)\n",
    "print(f\"Structural Validity: {validity_score:.3f}\")\n",
    "\n",
    "freshness_score = calc_freshness(raw_df)\n",
    "print(f\"Freshness: {freshness_score:.3f}\")\n",
    "\n",
    "base_dqs = calc_dqs(completeness_score, validity_score, freshness_score)\n",
    "print(f\"\\nBase DQS: {base_dqs:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm_header",
   "metadata": {},
   "source": [
    "## Step 4: LLM-Powered Semantic Validity Check\n",
    "\n",
    "Use OpenAI's o3-mini model to assess semantic validity on a sample of records.\n",
    "\n",
    "**Note:** This step costs approximately $0.20 per 100 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "llm_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LLM validity check on 10 records...\n",
      "   Sending request to OpenAI API...\n",
      "LLM analysis complete\n",
      "Average semantic validity: 0.750\n",
      "Detailed results saved to: llm_validity_20251013_115227.csv\n"
     ]
    }
   ],
   "source": [
    "def llm_validity_check(df, sample_size=LLM_SAMPLE_SIZE):\n",
    "    \"\"\"Run LLM-powered semantic validity check on sample.\"\"\"\n",
    "    print(f\"Running LLM validity check on {sample_size} records...\")\n",
    "    \n",
    "    # Check for API key\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"Warning: OPENAI_API_KEY not found in .env file\")\n",
    "        print(\"Skipping LLM validity check\")\n",
    "        return None, None\n",
    "    \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Random sample\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=LLM_SEED)\n",
    "    \n",
    "    # System prompt\n",
    "    system_prompt = \"\"\"You are a data quality analyst evaluating the VALIDITY of trucking company data records.\n",
    "\n",
    "For each record, assess the **trustworthiness** of the data using these criteria:\n",
    "- Company name plausibility (typos, placeholders, fake text like \"ABC Company\")\n",
    "- Plausibility of metrics (e.g., 150,000 miles/truck/year is reasonable; 850,000 is not)\n",
    "- Consistency between related fields (e.g., 2 drivers but 50 trucks is inconsistent)\n",
    "- Format issues (emails, phone numbers, or addresses that look invalid)\n",
    "\n",
    "Return your judgment as structured JSON with the following schema:\n",
    "[\n",
    "  {\n",
    "    \"record_id\": <index or dot_number>,\n",
    "    \"validity_score\": <float between 0 and 1>,\n",
    "    \"issues\": \"<short bullet summary of detected problems or 'None'>\",\n",
    "    \"summary_comment\": \"<2-sentence human-readable summary>\"\n",
    "  }\n",
    "]\n",
    "\n",
    "Scoring guidelines:\n",
    "- 1.0 = fully valid, realistic, consistent\n",
    "- 0.5 = minor issues or plausible but slightly off\n",
    "- 0.0 = clearly invalid, placeholder, or implausible\n",
    "\n",
    "Output only valid JSON.\n",
    "\"\"\"\n",
    "    \n",
    "    # Construct user prompt\n",
    "    records = sample_df.to_dict(orient=\"records\")\n",
    "    user_prompt = \"Evaluate the following records for data validity:\\n\\n\"\n",
    "    for rec in records:\n",
    "        user_prompt += f\"{rec}\\n\"\n",
    "    \n",
    "    # Call OpenAI API\n",
    "    print(\"   Sending request to OpenAI API...\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            seed=LLM_SEED,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        \n",
    "        # Parse JSON response\n",
    "        results = json.loads(response_text)\n",
    "        \n",
    "        # Calculate average semantic validity score\n",
    "        validity_scores = [r.get(\"validity_score\", 0) for r in results]\n",
    "        avg_semantic_validity = np.mean(validity_scores)\n",
    "        \n",
    "        print(f\"LLM analysis complete\")\n",
    "        print(f\"Average semantic validity: {avg_semantic_validity:.3f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = RESULTS_DIR / f\"llm_validity_{run_timestamp}.csv\"\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"Detailed results saved to: {output_path.name}\")\n",
    "        \n",
    "        return avg_semantic_validity, results_df\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"LLM did not return valid JSON\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM check: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Run LLM check\n",
    "semantic_validity_score, llm_results = llm_validity_check(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_header",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Final DQS and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "final_dqs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Validity Score: 0.851\n",
      "   (Structural: 0.952 + Semantic: 0.750)\n",
      "\n",
      "==================================================\n",
      "FINAL DATA QUALITY SCORE (DQS): 0.761\n",
      "==================================================\n",
      "   Completeness:  0.924\n",
      "   Validity:      0.851\n",
      "   Timeliness:    0.254\n"
     ]
    }
   ],
   "source": [
    "# Calculate final DQS (blend structural and semantic validity if available)\n",
    "if semantic_validity_score is not None:\n",
    "    # Average structural and semantic validity\n",
    "    combined_validity = (validity_score + semantic_validity_score) / 2\n",
    "    print(f\"\\nCombined Validity Score: {combined_validity:.3f}\")\n",
    "    print(f\"   (Structural: {validity_score:.3f} + Semantic: {semantic_validity_score:.3f})\")\n",
    "else:\n",
    "    combined_validity = validity_score\n",
    "    print(f\"\\nUsing Structural Validity Only: {combined_validity:.3f}\")\n",
    "\n",
    "# Calculate final DQS\n",
    "final_dqs = calc_dqs(completeness_score, combined_validity, freshness_score)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"FINAL DATA QUALITY SCORE (DQS): {final_dqs:.3f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"   Completeness:  {completeness_score:.3f}\")\n",
    "print(f\"   Validity:      {combined_validity:.3f}\")\n",
    "print(f\"   Timeliness:    {freshness_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history_header",
   "metadata": {},
   "source": [
    "## Step 6: Update Historical Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "save_history",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results to historical tracking...\n",
      "Appended to existing history (2 previous runs)\n",
      "Saved to: ../quality-scores/dqs_history.csv\n",
      "\n",
      "Historical DQS Trend:\n",
      "              run_date  final_dqs  completeness  combined_validity  timeliness\n",
      "0  2025-10-13 11:54:44     0.7097        1.0000             0.6416      0.2656\n",
      "1  2025-10-13 13:53:09     0.7549        0.9244             0.8359      0.2539\n",
      "2  2025-10-13 13:57:54     0.7609        0.9244             0.8509      0.2539\n"
     ]
    }
   ],
   "source": [
    "def append_to_history(results_dict, filepath=HISTORY_FILE):\n",
    "    \"\"\"Append current run results to historical tracking CSV.\"\"\"\n",
    "    # Create DataFrame for current run\n",
    "    current_run = pd.DataFrame([results_dict])\n",
    "    \n",
    "    # Load existing history if it exists\n",
    "    if filepath.exists():\n",
    "        history_df = pd.read_csv(filepath)\n",
    "        updated_df = pd.concat([history_df, current_run], ignore_index=True)\n",
    "        print(f\"Appended to existing history ({len(history_df)} previous runs)\")\n",
    "    else:\n",
    "        updated_df = current_run\n",
    "        print(f\"Created new history file\")\n",
    "    \n",
    "    # Save updated history\n",
    "    updated_df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved to: {filepath}\")\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "# Prepare results dictionary\n",
    "results = {\n",
    "    \"timestamp\": run_timestamp,\n",
    "    \"run_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"record_count\": len(raw_df),\n",
    "    \"completeness\": round(completeness_score, 4),\n",
    "    \"structural_validity\": round(validity_score, 4),\n",
    "    \"semantic_validity\": round(semantic_validity_score, 4) if semantic_validity_score else None,\n",
    "    \"combined_validity\": round(combined_validity, 4),\n",
    "    \"timeliness\": round(timeliness_score, 4),\n",
    "    \"final_dqs\": round(final_dqs, 4),\n",
    "    \"parquet_file\": parquet_path.name,\n",
    "    \"llm_sample_size\": LLM_SAMPLE_SIZE if semantic_validity_score else 0\n",
    "}\n",
    "\n",
    "# Save to history\n",
    "print(\"\\nSaving results to historical tracking...\")\n",
    "history_df = append_to_history(results)\n",
    "\n",
    "print(\"\\nHistorical DQS Trend:\")\n",
    "print(history_df[[\"run_date\", \"final_dqs\", \"completeness\", \"combined_validity\", \"timeliness\"]].tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pipeline execution complete! Key artifacts generated:\n",
    "- **Parquet file**: Latest dataset in optimized format\n",
    "- **DQS history**: Cumulative tracking of all runs\n",
    "- **LLM results**: Detailed semantic validity analysis (if run)\n",
    "\n",
    "To re-run this pipeline for new data, simply execute all cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DQS PIPELINE EXECUTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final DQS: 0.761\n",
      "Data saved: transportation_data_20251013_135544.parquet\n",
      "History file: dqs_history.csv\n",
      "Run completed: 2025-10-13 13:58:00\n",
      "\n",
      "To re-run for new data, execute all cells again.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DQS PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal DQS: {final_dqs:.3f}\")\n",
    "print(f\"Data saved: {parquet_path.name}\")\n",
    "print(f\"History file: {HISTORY_FILE.name}\")\n",
    "print(f\"Run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nTo re-run for new data, execute all cells again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
