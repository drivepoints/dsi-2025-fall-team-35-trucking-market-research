{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d499a661-05f7-4a32-9a11-ff43979636dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Union\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d77f6b4-7e59-4515-8a01-6b4348cef6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FARS + CRSS -> company summaries -> full census with metrics + simple exposure score + safety index\n",
    "# Exposure fields (your names):\n",
    "#   RECENT_MILEAGE  (miles)\n",
    "#   NBR_POWER_UNIT  (trucks)\n",
    "#   DRIVER_TOTAL    (drivers)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "# -------------------- IO & Normalization Helpers --------------------\n",
    "\n",
    "def _read_parquet_required(path: Union[str, Path]) -> pd.DataFrame:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required file: {p.resolve()}\")\n",
    "    return pd.read_parquet(p)\n",
    "\n",
    "def _stdcols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = pd.Index([str(c).strip().upper() for c in d.columns])\n",
    "    return d\n",
    "\n",
    "def _clean_usdot_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\").str.strip().str.replace(r\"^\\s*0+(?=\\d)\", \"\", regex=True)\n",
    "    return s.mask(s.isin([\"\", \"nan\", \"<NA>\"]))\n",
    "\n",
    "def _force_string_identifiers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    id_tokens = (\"VIN\", \"PLATE\", \"MCARR_I2\", \"DOT_NUMBER\", \"USDOT\")\n",
    "    d = df.copy()\n",
    "    for c in d.columns:\n",
    "        if any(tok in c.upper() for tok in id_tokens):\n",
    "            d[c] = d[c].astype(\"string\")\n",
    "    return d\n",
    "\n",
    "# -------------------- Standardization for merged crash rows --------------------\n",
    "\n",
    "def standardize_census_fields(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expected columns for merged crash rows:\n",
    "      DOT_NUMBER, LEGAL_NAME, DBA_NAME, SOURCE, LIKELY_AT_FAULT, FAULT_SCORE\n",
    "    \"\"\"\n",
    "    d = _stdcols(df)\n",
    "\n",
    "    if \"DOT_NUMBER\" not in d.columns and \"MCARR_I2\" in d.columns:\n",
    "        d[\"DOT_NUMBER\"] = d[\"MCARR_I2\"]\n",
    "    if \"DOT_NUMBER\" not in d.columns:\n",
    "        raise ValueError(\"DOT_NUMBER not found (and no MCARR_I2 to backfill).\")\n",
    "    d[\"DOT_NUMBER\"] = _clean_usdot_series(d[\"DOT_NUMBER\"])\n",
    "\n",
    "    if \"LEGAL_NAME\" not in d.columns:\n",
    "        for alt in (\"LEGALNAME\", \"CARRIER_LEGAL_NAME\"):\n",
    "            if alt in d.columns:\n",
    "                d[\"LEGAL_NAME\"] = d[alt]\n",
    "                break\n",
    "    if \"LEGAL_NAME\" not in d.columns:\n",
    "        d[\"LEGAL_NAME\"] = pd.NA\n",
    "\n",
    "    if \"DBA_NAME\" not in d.columns:\n",
    "        for alt in (\"DBANAME\", \"CARRIER_DBA_NAME\"):\n",
    "            if alt in d.columns:\n",
    "                d[\"DBA_NAME\"] = d[alt]\n",
    "                break\n",
    "    if \"DBA_NAME\" not in d.columns:\n",
    "        d[\"DBA_NAME\"] = pd.NA\n",
    "\n",
    "    for req in (\"SOURCE\", \"LIKELY_AT_FAULT\", \"FAULT_SCORE\"):\n",
    "        if req not in d.columns:\n",
    "            raise ValueError(f\"Missing required field: {req}\")\n",
    "\n",
    "    d[\"LIKELY_AT_FAULT\"] = pd.to_numeric(d[\"LIKELY_AT_FAULT\"], errors=\"coerce\")\n",
    "    d[\"FAULT_SCORE\"]     = pd.to_numeric(d[\"FAULT_SCORE\"], errors=\"coerce\")\n",
    "\n",
    "    return _force_string_identifiers(d)\n",
    "\n",
    "# -------------------- Full census backbone (2M+ rows) --------------------\n",
    "\n",
    "def standardize_census_backbone(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Minimal standardization for the RAW census snapshot (ALL carriers).\n",
    "    Ensures DOT_NUMBER, LEGAL_NAME, DBA_NAME exist; preserves exposure fields.\n",
    "    \"\"\"\n",
    "    d = _stdcols(df)\n",
    "\n",
    "    if \"DOT_NUMBER\" not in d.columns and \"MCARR_I2\" in d.columns:\n",
    "        d[\"DOT_NUMBER\"] = d[\"MCARR_I2\"]\n",
    "    if \"DOT_NUMBER\" not in d.columns:\n",
    "        raise ValueError(\"Raw census snapshot missing DOT_NUMBER/MCARR_I2.\")\n",
    "    d[\"DOT_NUMBER\"] = _clean_usdot_series(d[\"DOT_NUMBER\"])\n",
    "\n",
    "    if \"LEGAL_NAME\" not in d.columns:\n",
    "        for alt in (\"LEGALNAME\", \"CARRIER_LEGAL_NAME\"):\n",
    "            if alt in d.columns:\n",
    "                d[\"LEGAL_NAME\"] = d[alt]\n",
    "                break\n",
    "    if \"LEGAL_NAME\" not in d.columns:\n",
    "        d[\"LEGAL_NAME\"] = pd.NA\n",
    "\n",
    "    if \"DBA_NAME\" not in d.columns:\n",
    "        for alt in (\"DBANAME\", \"CARRIER_DBA_NAME\"):\n",
    "            if alt in d.columns:\n",
    "                d[\"DBA_NAME\"] = d[alt]\n",
    "                break\n",
    "    if \"DBA_NAME\" not in d.columns:\n",
    "        d[\"DBA_NAME\"] = pd.NA\n",
    "\n",
    "    return _force_string_identifiers(d)\n",
    "\n",
    "# -------------------- Exposure helpers & rates (uses your field names) --------------------\n",
    "\n",
    "def _first_existing(df: pd.DataFrame, candidates) -> Optional[str]:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _num_series(df: pd.DataFrame, col: Optional[str]) -> pd.Series:\n",
    "    if col is None or col not in df.columns:\n",
    "        return pd.Series(np.nan, index=df.index)\n",
    "    return pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "def add_exposure_rates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add exposure-adjusted crash rates and simple ratios to a census+metrics dataframe.\n",
    "    Uses your fields first: RECENT_MILEAGE, NBR_POWER_UNIT, DRIVER_TOTAL.\n",
    "\n",
    "    New columns added:\n",
    "      - accidents_per_truck\n",
    "      - accidents_per_driver\n",
    "      - accidents_per_million_miles\n",
    "\n",
    "    Existing (kept / created):\n",
    "      - rate_per_100_trucks, rate_at_fault_per_100_trucks\n",
    "      - rate_per_100_drivers, rate_at_fault_per_100_drivers\n",
    "      - rate_per_1m_miles,   rate_at_fault_per_1m_miles\n",
    "      - fars_rate_per_*, crss_rate_per_*\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    pu_col    = _first_existing(out, [\"NBR_POWER_UNIT\",\"POWER_UNITS\",\"POWER_UNITS_CNT\",\"POWER_UNIT\",\"PU\",\"TRUCKS\"])\n",
    "    drv_col   = _first_existing(out, [\"DRIVER_TOTAL\",\"DRIVERS\",\"DRIVERCNT\",\"TOTAL_DRIVERS\"])\n",
    "    miles_col = _first_existing(out, [\"RECENT_MILEAGE\",\"MCS150_MILES\",\"MCS150MILES\",\"TOTAL_MILES\",\"VMT\",\"ANNUAL_VMT\"])\n",
    "\n",
    "    pu    = _num_series(out, pu_col).where(lambda s: s > 0)\n",
    "    drv   = _num_series(out, drv_col).where(lambda s: s > 0)\n",
    "    miles = _num_series(out, miles_col).where(lambda s: s > 0)\n",
    "\n",
    "    tot    = pd.to_numeric(out.get(\"total_crashes\", np.nan), errors=\"coerce\")\n",
    "    tot_af = pd.to_numeric(out.get(\"total_at_fault_crashes\", np.nan), errors=\"coerce\")\n",
    "\n",
    "    # ---- Per-100 / per-1M (total & at-fault) ----\n",
    "    out[\"rate_per_100_trucks\"]           = (tot    / pu    * 100).where(pu.notna())\n",
    "    out[\"rate_at_fault_per_100_trucks\"]  = (tot_af / pu    * 100).where(pu.notna())\n",
    "\n",
    "    out[\"rate_per_100_drivers\"]          = (tot    / drv   * 100).where(drv.notna())\n",
    "    out[\"rate_at_fault_per_100_drivers\"] = (tot_af / drv   * 100).where(drv.notna())\n",
    "\n",
    "    out[\"rate_per_1m_miles\"]             = (tot    / miles * 1_000_000).where(miles.notna())\n",
    "    out[\"rate_at_fault_per_1m_miles\"]    = (tot_af / miles * 1_000_000).where(miles.notna())\n",
    "\n",
    "    # ---- Simple ratios (at-fault) ----\n",
    "    out[\"accidents_per_truck\"]         = (tot_af / pu).where(pu.notna())\n",
    "    out[\"accidents_per_driver\"]        = (tot_af / drv).where(drv.notna())\n",
    "    out[\"accidents_per_million_miles\"] = out[\"rate_at_fault_per_1m_miles\"]\n",
    "\n",
    "    # ---- Source-specific rates (FARS/CRSS) ----\n",
    "    for src in (\"fars\", \"crss\"):\n",
    "        src_tot = pd.to_numeric(out.get(f\"{src}_total\", np.nan), errors=\"coerce\")\n",
    "        if src_tot.notna().any():\n",
    "            out[f\"{src}_rate_per_100_trucks\"]  = (src_tot / pu    * 100).where(pu.notna())\n",
    "            out[f\"{src}_rate_per_100_drivers\"] = (src_tot / drv   * 100).where(drv.notna())\n",
    "            out[f\"{src}_rate_per_1m_miles\"]    = (src_tot / miles * 1_000_000).where(miles.notna())\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------- Combined exposure score + Safety Index --------------------\n",
    "\n",
    "def add_simple_exposure_score(\n",
    "    df: pd.DataFrame,\n",
    "    miles_scale: float = 1_000_000,\n",
    "    safety_cap_pct: float = 99.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    exposure_score = trucks + drivers + miles / miles_scale\n",
    "    accident_exposure_score = total_at_fault_crashes / exposure_score\n",
    "    safety_index = 100 * (1 - accident_exposure_score / pXX_cap), clipped to [0,100]\n",
    "      where pXX_cap is the safety_cap_pct percentile of accident_exposure_score (default 99th)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    trucks  = pd.to_numeric(out.get(\"NBR_POWER_UNIT\"),  errors=\"coerce\").astype(\"float64\")\n",
    "    drivers = pd.to_numeric(out.get(\"DRIVER_TOTAL\"),    errors=\"coerce\").astype(\"float64\")\n",
    "    miles   = pd.to_numeric(out.get(\"RECENT_MILEAGE\"),  errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "    exposure = trucks.fillna(0.0) + drivers.fillna(0.0) + (miles.fillna(0.0) / float(miles_scale))\n",
    "    exposure = exposure.where(exposure > 0)\n",
    "\n",
    "    out[\"exposure_score\"] = exposure.astype(\"float64\")\n",
    "\n",
    "    at_fault = pd.to_numeric(out.get(\"total_at_fault_crashes\"), errors=\"coerce\").astype(\"float64\")\n",
    "    score = (at_fault / out[\"exposure_score\"]).replace([np.inf, -np.inf], np.nan).astype(\"float64\")\n",
    "    out[\"accident_exposure_score\"] = score\n",
    "\n",
    "    # Safety Index (0–100, higher = safer), capped by chosen percentile to avoid outlier collapse\n",
    "    if score.notna().any():\n",
    "        cap = np.nanpercentile(score, safety_cap_pct)\n",
    "        denom = cap if np.isfinite(cap) and cap > 0 else np.nanmax(score.values)\n",
    "        if denom and denom > 0:\n",
    "            si = 100 * (1 - (score / denom))\n",
    "            out[\"safety_index\"] = np.clip(si, 0, 100)\n",
    "        else:\n",
    "            out[\"safety_index\"] = np.nan\n",
    "    else:\n",
    "        out[\"safety_index\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "# -------------------- Build per-source & overall summaries from merged rows --------------------\n",
    "\n",
    "def build_company_summary(\n",
    "    fars_merged_path: Union[str, Path] = \"census_with_fars.parquet\",\n",
    "    crss_merged_path: Union[str, Path] = \"census_with_crss.parquet\",\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Input: FARS- and CRSS-merged-to-census parquet files.\n",
    "    Output:\n",
    "      - per_source: DOT x SOURCE aggregates\n",
    "      - overall: DOT-level aggregates (with fars_* and crss_* columns)\n",
    "      - both: stacked crash rows (FARS + CRSS)\n",
    "    \"\"\"\n",
    "    fars = standardize_census_fields(_read_parquet_required(fars_merged_path))\n",
    "    crss = standardize_census_fields(_read_parquet_required(crss_merged_path))\n",
    "\n",
    "    both = pd.concat([fars, crss], ignore_index=True)\n",
    "    both = both[both[\"DOT_NUMBER\"].notna()].copy()\n",
    "\n",
    "    both[\"MATCHED_FLAG\"] = (both[\"SOURCE\"].notna()).astype(int)\n",
    "    both[\"AT_FAULT_FLAG\"] = np.where(\n",
    "        both[\"MATCHED_FLAG\"] == 1,\n",
    "        (both[\"LIKELY_AT_FAULT\"] == 1).astype(\"float\"),\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "    per_source = (\n",
    "        both.groupby([\"DOT_NUMBER\", \"LEGAL_NAME\", \"DBA_NAME\", \"SOURCE\"], dropna=False)\n",
    "            .agg(\n",
    "                total_crashes          = (\"MATCHED_FLAG\", \"sum\"),\n",
    "                total_at_fault_crashes = (\"AT_FAULT_FLAG\", \"sum\"),\n",
    "                pct_at_fault           = (\"AT_FAULT_FLAG\", \"mean\"),\n",
    "                mean_fault_score       = (\"FAULT_SCORE\", \"mean\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "    )\n",
    "    per_source[\"total_at_fault_crashes\"] = per_source[\"total_at_fault_crashes\"].fillna(0).astype(int)\n",
    "\n",
    "    overall = (\n",
    "        both.groupby([\"DOT_NUMBER\", \"LEGAL_NAME\", \"DBA_NAME\"], dropna=False)\n",
    "            .agg(\n",
    "                total_crashes          = (\"MATCHED_FLAG\", \"sum\"),\n",
    "                total_at_fault_crashes = (\"AT_FAULT_FLAG\", \"sum\"),\n",
    "                mean_fault_score       = (\"FAULT_SCORE\", \"mean\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "    )\n",
    "    overall[\"total_at_fault_crashes\"] = overall[\"total_at_fault_crashes\"].fillna(0).astype(int)\n",
    "\n",
    "    fars_split = per_source.loc[\n",
    "        per_source[\"SOURCE\"] == \"FARS\",\n",
    "        [\"DOT_NUMBER\", \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\"],\n",
    "    ]\n",
    "    fars_split.columns = [\"DOT_NUMBER\", \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\"]\n",
    "\n",
    "    crss_split = per_source.loc[\n",
    "        per_source[\"SOURCE\"] == \"CRSS\",\n",
    "        [\"DOT_NUMBER\", \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\"],\n",
    "    ]\n",
    "    crss_split.columns = [\"DOT_NUMBER\", \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\"]\n",
    "\n",
    "    overall = (\n",
    "        overall\n",
    "        .merge(fars_split, on=\"DOT_NUMBER\", how=\"left\")\n",
    "        .merge(crss_split, on=\"DOT_NUMBER\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    for col in [\"fars_total\", \"fars_at_fault\", \"crss_total\", \"crss_at_fault\"]:\n",
    "        overall[col] = overall[col].fillna(0).astype(int)\n",
    "\n",
    "    overall[\"pct_at_fault\"] = np.where(\n",
    "        overall[\"total_crashes\"] > 0,\n",
    "        overall[\"total_at_fault_crashes\"] / overall[\"total_crashes\"],\n",
    "        np.nan,\n",
    "    )\n",
    "\n",
    "    column_order = [\n",
    "        \"DOT_NUMBER\", \"LEGAL_NAME\", \"DBA_NAME\",\n",
    "        \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\",\n",
    "        \"mean_fault_score\",\n",
    "        \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\",\n",
    "    ]\n",
    "    remaining = [c for c in overall.columns if c not in column_order]\n",
    "    overall = overall[column_order + remaining]\n",
    "\n",
    "    overall = overall.sort_values(\n",
    "        [\"total_at_fault_crashes\", \"total_crashes\", \"pct_at_fault\"],\n",
    "        ascending=[False, False, False],\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    return per_source, overall, both\n",
    "\n",
    "# -------------------- Full census with metrics (matched-only fallback) --------------------\n",
    "\n",
    "def make_full_census_with_metrics(combined_rows: pd.DataFrame, overall: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a census-with-metrics frame from merged crash rows (FARS+CRSS).\n",
    "    \"\"\"\n",
    "    crash_cols = {\"SOURCE\", \"LIKELY_AT_FAULT\", \"FAULT_SCORE\", \"MATCHED_FLAG\", \"AT_FAULT_FLAG\"}\n",
    "    census_cols = [c for c in combined_rows.columns if c not in crash_cols]\n",
    "    census_backbone = combined_rows[census_cols].drop_duplicates().copy()\n",
    "\n",
    "    metric_cols = [\n",
    "        \"DOT_NUMBER\",\n",
    "        \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\", \"mean_fault_score\",\n",
    "        \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\",\n",
    "    ]\n",
    "    metrics = overall[metric_cols].copy()\n",
    "\n",
    "    out = census_backbone.merge(metrics, on=\"DOT_NUMBER\", how=\"left\")\n",
    "\n",
    "    for col in [\n",
    "        \"total_crashes\", \"total_at_fault_crashes\",\n",
    "        \"fars_total\", \"fars_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\",\n",
    "    ]:\n",
    "        if col in out.columns:\n",
    "            out[col] = out[col].fillna(0).astype(int)\n",
    "\n",
    "    out = add_exposure_rates(out)\n",
    "    out = add_simple_exposure_score(out)  # adds exposure_score, accident_exposure_score, safety_index\n",
    "    return out\n",
    "\n",
    "# -------------------- Preferred: build full census from raw backbone --------------------\n",
    "\n",
    "def make_full_census_with_metrics_from_base(\n",
    "    census_base_path: Union[str, Path],\n",
    "    overall: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build census-with-metrics from a raw census backbone parquet (ALL carriers).\n",
    "    \"\"\"\n",
    "    base = _read_parquet_required(census_base_path)\n",
    "    base = standardize_census_backbone(base)\n",
    "\n",
    "    metric_cols = [\n",
    "        \"DOT_NUMBER\",\n",
    "        \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\", \"mean_fault_score\",\n",
    "        \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\",\n",
    "    ]\n",
    "    metrics = overall[metric_cols].copy()\n",
    "\n",
    "    out = base.merge(metrics, on=\"DOT_NUMBER\", how=\"left\")\n",
    "\n",
    "    for col in [\n",
    "        \"total_crashes\", \"total_at_fault_crashes\",\n",
    "        \"fars_total\", \"fars_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\",\n",
    "    ]:\n",
    "        if col in out.columns:\n",
    "            out[col] = out[col].fillna(0).astype(int)\n",
    "\n",
    "    out = add_exposure_rates(out)\n",
    "    out = add_simple_exposure_score(out)  # adds exposure_score, accident_exposure_score, safety_index\n",
    "    return out\n",
    "\n",
    "# -------------------- Metrics-only export (single file) --------------------\n",
    "\n",
    "def export_metrics_from_census_csv(\n",
    "    overall: pd.DataFrame,\n",
    "    census_csv_path: Union[str, Path],\n",
    "    output_path: Union[str, Path] = \"fars_crss_metrics_only.parquet\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DOT-level metrics table from:\n",
    "      - `overall`: crash metrics per DOT (from build_company_summary)\n",
    "      - `census_csv_path`: raw census CSV with exposure fields\n",
    "\n",
    "    Writes a single file (Parquet by default) at `output_path` with:\n",
    "      - DOT_NUMBER\n",
    "      - exposure fields: NBR_POWER_UNIT, DRIVER_TOTAL, RECENT_MILEAGE\n",
    "      - all accident / rate / safety metrics\n",
    "    and returns the resulting DataFrame.\n",
    "    \"\"\"\n",
    "    # 1) Load census CSV and standardize\n",
    "    census_csv_path = Path(census_csv_path)\n",
    "    base = pd.read_csv(census_csv_path, low_memory=False)\n",
    "    base = standardize_census_backbone(base)\n",
    "\n",
    "    # 2) Attach crash metrics from `overall`\n",
    "    metric_cols = [\n",
    "        \"DOT_NUMBER\",\n",
    "        \"total_crashes\", \"total_at_fault_crashes\", \"pct_at_fault\", \"mean_fault_score\",\n",
    "        \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\",\n",
    "    ]\n",
    "    metrics = overall[metric_cols].copy()\n",
    "\n",
    "    out = base.merge(metrics, on=\"DOT_NUMBER\", how=\"left\")\n",
    "\n",
    "    # Fill counts with zeros where appropriate\n",
    "    for col in [\n",
    "        \"total_crashes\", \"total_at_fault_crashes\",\n",
    "        \"fars_total\", \"fars_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\",\n",
    "    ]:\n",
    "        if col in out.columns:\n",
    "            out[col] = out[col].fillna(0).astype(int)\n",
    "\n",
    "    # 3) Add exposure-based rates & scores\n",
    "    out = add_exposure_rates(out)\n",
    "    out = add_simple_exposure_score(out)  # exposure_score, accident_exposure_score, safety_index\n",
    "\n",
    "    # 4) Keep only DOT + exposure + accident/rate/safety columns\n",
    "    cols_to_keep = [\n",
    "        \"DOT_NUMBER\",\n",
    "        # crash counts & fault metrics\n",
    "        \"total_crashes\", \"total_at_fault_crashes\",\n",
    "        \"pct_at_fault\", \"mean_fault_score\",\n",
    "        \"fars_total\", \"fars_at_fault\", \"fars_pct_at_fault\",\n",
    "        \"crss_total\", \"crss_at_fault\", \"crss_pct_at_fault\",\n",
    "        # exposure-adjusted rates\n",
    "        \"rate_per_100_trucks\", \"rate_at_fault_per_100_trucks\",\n",
    "        \"rate_per_100_drivers\", \"rate_at_fault_per_100_drivers\",\n",
    "        \"rate_per_1m_miles\", \"rate_at_fault_per_1m_miles\",\n",
    "        # simple ratios\n",
    "        \"accidents_per_truck\", \"accidents_per_driver\", \"accidents_per_million_miles\",\n",
    "        # source-specific rates\n",
    "        \"fars_rate_per_100_trucks\", \"fars_rate_per_100_drivers\", \"fars_rate_per_1m_miles\",\n",
    "        \"crss_rate_per_100_trucks\", \"crss_rate_per_100_drivers\", \"crss_rate_per_1m_miles\",\n",
    "        # composite scores\n",
    "        \"exposure_score\", \"accident_exposure_score\", \"safety_index\",\n",
    "    ]\n",
    "    cols_to_keep = [c for c in cols_to_keep if c in out.columns]\n",
    "\n",
    "    metrics_only = out[cols_to_keep].copy()\n",
    "    metrics_only = _force_string_identifiers(metrics_only)\n",
    "\n",
    "    # 5) Export a single file\n",
    "    output_path = Path(output_path)\n",
    "    metrics_only.to_parquet(output_path, index=False)\n",
    "\n",
    "    print(\"\\nSaved metrics-only crash+exposure table:\")\n",
    "    print(\" \", output_path.resolve())\n",
    "\n",
    "    return metrics_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a54082-c1c2-4f56-82f9-2331a6cc0efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved metrics-only crash+exposure table:\n",
      "  /Users/bens1858/Desktop/Columbia/Fall 2025/Capstone/fars_crss_census.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOT_NUMBER</th>\n",
       "      <th>total_crashes</th>\n",
       "      <th>total_at_fault_crashes</th>\n",
       "      <th>pct_at_fault</th>\n",
       "      <th>mean_fault_score</th>\n",
       "      <th>fars_total</th>\n",
       "      <th>fars_at_fault</th>\n",
       "      <th>fars_pct_at_fault</th>\n",
       "      <th>crss_total</th>\n",
       "      <th>crss_at_fault</th>\n",
       "      <th>...</th>\n",
       "      <th>accidents_per_million_miles</th>\n",
       "      <th>fars_rate_per_100_trucks</th>\n",
       "      <th>fars_rate_per_100_drivers</th>\n",
       "      <th>fars_rate_per_1m_miles</th>\n",
       "      <th>crss_rate_per_100_trucks</th>\n",
       "      <th>crss_rate_per_100_drivers</th>\n",
       "      <th>crss_rate_per_1m_miles</th>\n",
       "      <th>exposure_score</th>\n",
       "      <th>accident_exposure_score</th>\n",
       "      <th>safety_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098901</td>\n",
       "      <td>25.0</td>\n",
       "      <td>142.857143</td>\n",
       "      <td>95.007000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.009949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.119162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.015922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  DOT_NUMBER  total_crashes  total_at_fault_crashes  pct_at_fault  \\\n",
       "0          1              1                       0           0.0   \n",
       "1      10000              0                       0           NaN   \n",
       "2    1000000              0                       0           NaN   \n",
       "3    1000002              0                       0           NaN   \n",
       "4    1000004              0                       0           NaN   \n",
       "\n",
       "   mean_fault_score  fars_total  fars_at_fault  fars_pct_at_fault  crss_total  \\\n",
       "0               0.0           0              0                NaN           1   \n",
       "1               NaN           0              0                NaN           0   \n",
       "2               NaN           0              0                NaN           0   \n",
       "3               NaN           0              0                NaN           0   \n",
       "4               NaN           0              0                NaN           0   \n",
       "\n",
       "   crss_at_fault  ...  accidents_per_million_miles  fars_rate_per_100_trucks  \\\n",
       "0              0  ...                          0.0                       0.0   \n",
       "1              0  ...                          0.0                       0.0   \n",
       "2              0  ...                          0.0                       0.0   \n",
       "3              0  ...                          0.0                       0.0   \n",
       "4              0  ...                          NaN                       0.0   \n",
       "\n",
       "   fars_rate_per_100_drivers  fars_rate_per_1m_miles  \\\n",
       "0                        0.0                     0.0   \n",
       "1                        0.0                     0.0   \n",
       "2                        0.0                     0.0   \n",
       "3                        0.0                     0.0   \n",
       "4                        0.0                     NaN   \n",
       "\n",
       "   crss_rate_per_100_trucks  crss_rate_per_100_drivers  \\\n",
       "0                  1.098901                       25.0   \n",
       "1                  0.000000                        0.0   \n",
       "2                  0.000000                        0.0   \n",
       "3                  0.000000                        0.0   \n",
       "4                  0.000000                        0.0   \n",
       "\n",
       "   crss_rate_per_1m_miles  exposure_score  accident_exposure_score  \\\n",
       "0              142.857143       95.007000                      0.0   \n",
       "1                0.000000        2.009949                      0.0   \n",
       "2                0.000000        3.119162                      0.0   \n",
       "3                0.000000        2.015922                      0.0   \n",
       "4                     NaN        3.000000                      0.0   \n",
       "\n",
       "   safety_index  \n",
       "0         100.0  \n",
       "1         100.0  \n",
       "2         100.0  \n",
       "3         100.0  \n",
       "4         100.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Build crash summaries from your FARS/CRSS + census merged files\n",
    "per_source, overall, combined_rows = build_company_summary(\n",
    "    fars_merged_path=\"census_with_fars.parquet\",      # update if different\n",
    "    crss_merged_path=\"census_with_crss.parquet\",\n",
    ")\n",
    "\n",
    "# 2. Create the single DOT-level metrics file using your CSV census\n",
    "metrics_only = export_metrics_from_census_csv(\n",
    "    overall=overall,\n",
    "    census_csv_path=\"SMS_Input_-_Motor_Carrier_Census_Information_20250919.csv\",\n",
    "    output_path=\"fars_crss_census.parquet\",\n",
    ")\n",
    "\n",
    "metrics_only.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
