{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RZxJt40Q7GcA"
      },
      "outputs": [],
      "source": [
        "# === Imports: Core libraries and ML components used throughout the pipeline ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Apply Production Model to Full Dataset\n",
        "\n",
        "We preprocess the full dataset to match the exact training schema, apply the\n",
        "production target-encoding maps, enforce column alignment, and generate ML scores\n",
        "for downstream dashboard integration.\n"
      ],
      "metadata": {
        "id": "TZA1bVXb8U93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Prepare sample subset from full dataset for ML score generation ===\n",
        "cargo = pd.read_parquet('cargo_multi_hot_fast.parquet')\n",
        "df_full = pd.read_parquet(\"transportation_data_20250917_222245.parquet\")\n",
        "df_pred = df_full.copy()\n",
        "df_pred = df_pred.merge(cargo, on=\"dot_number\", how=\"left\")\n",
        "dot_numbers_raw = df_pred['dot_number'].copy()"
      ],
      "metadata": {
        "id": "x1CkYMMch4d_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pickle.load(open(\"final_model.pkl\", \"rb\"))\n",
        "feature_cols = pickle.load(open(\"feature_cols.pkl\", \"rb\"))\n",
        "te_maps = pickle.load(open(\"te_maps.pkl\", \"rb\"))\n",
        "target_cols = pickle.load(open(\"target_cols.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "031f8Qk0vUUu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Apply production-ready target encoding maps to new prediction data ===\n",
        "def apply_final_te(df_pred, te_maps, target_cols):\n",
        "    \"\"\"\n",
        "    Apply final (full-data) target encoding mappings to a new dataframe.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_pred : pandas.DataFrame\n",
        "        Input dataframe for prediction.\n",
        "    te_maps : dict\n",
        "        Dictionary of target-encoding maps learned from full training data.\n",
        "    target_cols : list\n",
        "        List of categorical columns that require target encoding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df2 : pandas.DataFrame\n",
        "        Dataframe with target-encoded columns appended.\n",
        "    \"\"\"\n",
        "    df2 = df_pred.copy()\n",
        "    for col in target_cols:\n",
        "        mapping = te_maps[col]\n",
        "        df2[col + \"_TE\"] = df2[col].astype(str).map(mapping).fillna(mapping[\"__GLOBAL__\"])\n",
        "    return df2"
      ],
      "metadata": {
        "id": "Fmw07JjFDz3Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Preprocess prediction data to align with the training feature schema ===\n",
        "\n",
        "# Drop the same identifier and non-predictive columns used in training\n",
        "drop_cols = [\n",
        "    'legal_name', 'dba_name', 'telephone', 'fax', 'email_address',\n",
        "    'phy_street', 'phy_city', 'phy_zip', 'phy_country',\n",
        "    'mailing_street', 'mailing_city', 'mailing_zip', 'mailing_country'\n",
        "]\n",
        "df_pred = df_pred.drop(columns=[c for c in drop_cols if c in df_pred.columns], errors='ignore')\n",
        "\n",
        "# Convert date fields to numerical 'days since' representation\n",
        "for col in ['mcs150_date', 'add_date']:\n",
        "    if col in df_pred.columns:\n",
        "        df_pred[col] = pd.to_datetime(df_pred[col], errors='coerce')\n",
        "        df_pred[col] = (pd.Timestamp.today() - df_pred[col]).dt.days\n",
        "\n",
        "# Normalize boolean-like fields into binary 0/1 format\n",
        "bool_cols = [\n",
        "    'authorized_for_hire', 'exempt_for_hire', 'private_only', 'private_property',\n",
        "    'private_passenger_business', 'private_passenger_nonbusiness', 'migrant', 'us_mail',\n",
        "    'federal_government', 'state_government', 'local_government',\n",
        "    'indian_tribe', 'op_other', 'pc_flag', 'hm_flag'\n",
        "]\n",
        "\n",
        "for col in bool_cols:\n",
        "    if col in df_pred.columns:\n",
        "        df_pred[col] = (\n",
        "            df_pred[col]\n",
        "            .astype(str)\n",
        "            .str.strip()\n",
        "            .str.lower()\n",
        "            .map({'true': 1, 'false': 0, '1': 1, '0': 0})\n",
        "            .fillna(0)\n",
        "            .astype(int)\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t2RUrXgh5pI",
        "outputId": "876651c7-300d-4042-b7b7-6aa5b528e5ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1460774878.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_pred[col] = pd.to_datetime(df_pred[col], errors='coerce')\n",
            "/tmp/ipython-input-1460774878.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_pred[col] = pd.to_datetime(df_pred[col], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Apply final target-encoding mappings to prediction data ===\n",
        "df_pred2 = apply_final_te(df_pred, te_maps, target_cols)\n",
        "df_pred2 = df_pred2.drop(columns=target_cols)"
      ],
      "metadata": {
        "id": "_P3yf6UAGZJi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-hot encode carrier_operation in prediction data (consistent with training schema) ===\n",
        "if 'carrier_operation' in df_pred2.columns:\n",
        "    co_ohe_pred = pd.get_dummies(\n",
        "        df_pred2['carrier_operation'].astype(str).fillna(\"MISSING\"),\n",
        "        prefix='co'\n",
        "    )\n",
        "    df_pred2 = pd.concat(\n",
        "        [df_pred2.drop(columns=['carrier_operation']), co_ohe_pred],\n",
        "        axis=1\n",
        "    )"
      ],
      "metadata": {
        "id": "rDsQyUKvVHQ7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Align prediction feature columns to the exact training feature schema ===\n",
        "df_pred2 = df_pred2.fillna(0)\n",
        "\n",
        "for col in feature_cols:\n",
        "    if col not in df_pred2.columns:\n",
        "        df_pred2[col] = 0\n",
        "\n",
        "extra_cols = [c for c in df_pred2.columns if c not in feature_cols]\n",
        "df_pred2 = df_pred2.drop(columns=extra_cols, errors='ignore')\n",
        "\n",
        "float_cols = df_pred2.select_dtypes(include=[\"float64\"]).columns\n",
        "df_pred2[float_cols] = df_pred2[float_cols].astype(\"float32\")\n",
        "\n",
        "\n",
        "df_pred2 = df_pred2[feature_cols]"
      ],
      "metadata": {
        "id": "8OUyzEb4ZHOQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Generate ML scores using the final production logistic regression model ===\n",
        "df_pred2[\"ml_score\"] = model.predict_proba(df_pred2)[:, 1]\n",
        "\n",
        "\n",
        "df_output = pd.DataFrame({\n",
        "    \"dot_number\": dot_numbers_raw,\n",
        "    \"ml_score\": df_pred2[\"ml_score\"]\n",
        "})\n",
        "\n",
        "df_output.to_csv(\"company_ml_scores.csv\", index=False)"
      ],
      "metadata": {
        "id": "7wMhXURFh5lA"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}